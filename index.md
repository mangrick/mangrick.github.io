---
layout: homepage
---

## About Me

I am a computer scientist at Johns Hopkins University, working at the intersection of neuroscience, machine learning, and spoken language technologies for the development of speech Brain-Computer Interfaces (BCIs). My research focuses on decoding neural signals associated with different speaking modalities, with the goal of restoring communication capabilities for individuals who have lost their voice due to neurodegenerative diseases, such as ALS. 

My work combines advanced deep learning with invasive neuroimaging techniques to create more intuitive and effective ways for the brain to communicate directly with computers. I'm particularly interested in building real-time decoding algorithms that translate neural activity into intelligible speech with minimal latency to allow for immediate and continuous acoustic feedback. This involves tackling challenges in digital signal processing, neural network architecture design, and the optimization of BCI systems for clinical applications. 

I am currently conducting research for the CortiCom project – a clinical trial spanning national and international institutions investigating the safety and efficacy of chronic ECoG implants for long-term BCI control. Here, we evaluate the stability of neural signals and the reliability of BCI control over extended periods of time, while developing robust decoding algorithms that translate attempted speech or motor movements into text or synthesized speech.

Feel free to explore ongoing projects, my publications, or other resources shared here. 

## Research Interests

- **Speech technologies:** Speech synthesis, automatic speech recognition 
- **Deep Learning:** Convolutional neural networks, recurrent neural networks, transformers, transfer learning

## Projects

- **CortiCom:** The primary goal of the CortiCom clinical trial is to evaluate the safety and efficacy of long-term electrode implantation for BCI control.  This entails clinical oversight and assessment of the long-term implantation of the electrodes and signal acquisition hardware, as well as the development, testing, and assessment of new BCI paradigms and control schemes to facilitate communication using the BCI. The two stated aims of CortiCom are: (1) Demonstrate efficient and stable control of essential BCI functions (e.g., initiate BCI, call caregiver, and BCI menu navigation), and (2) Demonstrate efficient and stable operation of a keyword-based speech BCI.

- **ADSPEED:** In the „ADaptive Low-Latency SPEEch Decoding and synthesis using intracranial signals“ (ADSPEED) project, we tackle three of these challenges to advance the technology that has so far been developed for individuals with healthy speech production functions towards its intended target users (e.g. ALS patients). Here, the ADSPEED project focuses on synthetizing natural speech via systems that rely on captured neural processes of imagined speech. To accomplish this, the ADSPEED team will address the decoding of imagined speech in real-time to allow user and system to co-adapt via a continuous neurofeedback. Thus, fundamental challenges for a natural and practical communication device that generates audible speech from brain activity data in real-time will be studied. <br> In this regard, ADSPEED constitutes a German-US collaboration in the field of computational neuroscience and specifically addresses the following four research thrusts: (1) The training of synthesis techniques even when time-aligned data between neural activities and speech are not available. (2) The investigation of online synthesis methods based on previous work to decode imagined speech and enable user adaptation via a continuous auditory feedback. (2) The development of techniques regarding co-adaptation between user and system. (3) A proof-of-concept study of a neuroprosthesis based on imagined speech via a co-adaptive online synthesis system without time-aligned data for training.

- **RESPONSE:** This project investigated the neural processes of spontaneous and imagined speech production using Electrocorticography (ECoG) which measures electrical activity directly from the brain surface and covers an area large enough to provide insights about widespread networks for speech production and understanding, while simultaneously providing localized information for decoding nuanced aspects of the underlying speech processes. In conjunction with in-depth analysis of the recorded neural signals, the researchers will apply customized ECoG-based automatic speech recognition (ASR) techniques to facilitate the analysis of the large amount of phones occurring in continuous speech. Ultimately, the project aims to define fundamental units of continuous speech production and understanding, illustrate functional differences between these units, and demonstrate that representations of spontaneous speech can be synthesized directly from the neural recordings.

{% include_relative _includes/publications.md %}
